---
title: "Simulación de ítems con distribuciones no-normales y su efecto en el análisis factorial exploratorio mediante correlaciones teracóricas/policóricas"
authors:
  - name: Brian Norman Peña-Calero
    orcid: 0000-0002-1073-9306
    affiliation: Universidad Complutense de Madrid (UCM)
    email: 
    roles: 
      - Researcher
      - Analyst
    corresponding: true
date: last-modified
date-format: "DD MMMM, YYYY"
lang: es
lightbox: true
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

# Condiciones de simulacion

```{r}
library(tidyverse)
```

- Número de ítems: 
  + 1 Factor: 
    * 5 ítems: cargas superior a 0.60 y cargas entre 0.30 a 0.70
    * 8 ítems: cargas superior a 0.60 y cargas entre 0.30 a 0.70
  + 2 Factores: 5 ítems por factor
  
  
Para la definición de los umbrales se realizó una adaptación de lo sugerido en @yang2015:

- Umbrales simétricos:
  + 2 categorías: $\tau_1 = 0$
  + 5 categorías: $\tau_1 = -1.5$, $\tau_2 = -0.5$, $\tau_3 = 0.5$, $\tau_4 = 1.5$
- Umbrales asimétricos:
  + 2 categorías: $tau_1 = -1$
  + 5 categorías: $\tau_1 = 0$, $\tau_2 = -0.5$, $\tau_3 = -1.25$, $\tau_4 = -1.75$


```{r}
r_unif_seed <- function(n, min, max, seed = 123) {
  set.seed(seed)
  round(runif(n, min, max))
}


loading_sim <- function(n_items, type_loadings, seed = 123) {
  
  if (type_loadings == "high") {
    min_loading <-  0.6
    max_loading <- 0.9
  } else if (type_loadings == "mid") {
    min_loading <-  0.3
    max_loading <- 0.7
  }
  
  set.seed(seed)
  round(runif(n_items, min_loading, max_loading), 3)
}

threshold_sim <- function(n_items, type_threshold, n_cat, seed = 123) {
  set.seed(seed)
  
  # Definir los umbrales base según el tipo y número de categorías
  base_thresholds <- switch(
    type_threshold,
    "symetric" = if (n_cat == 2) c(0) else c(-1.5, -0.5, 0.5, 1.5),
    "asym" = if (n_cat == 2) c(-1) else c(0, -0.5, -1.25, -1.75),
    stop("Invalid type_threshold")
  )
  
  # Agregar ruido a los umbrales base para cada ítem
  thresholds_list <- replicate(n_items, {
    sapply(base_thresholds, function(threshold) round(threshold + runif(1, -0.05, 0.05), 3))
  }, simplify = FALSE)
  
  return(thresholds_list)
}
```


```{r}
sim_conditions <- expand_grid(
  n_size = c(200, 500, 1000),
  n_factor = 1,
  distribution = c("MVN", "Non-MVN"),
  n_items = c(5, 8),
  type_loadings = c("high", "mid"),
  n_cat = c(2, 5),
  type_threshold = c("symetric", "asym")
)

sim_conditions <- sim_conditions |> 
  mutate(
    # Crear un identificador único para las combinaciones relevantes
    combo_id = paste(n_items, type_loadings, sep = "_"),
    # Asignar una semilla única a cada combinación
    unique_seed = as.integer(factor(combo_id)) + 100  # Generar semillas únicas replicables
  ) |> 
  group_by(combo_id) |> 
  mutate(
    # Usar la semilla única para generar valores aleatorios
    seed_loadings = r_unif_seed(1, 1, 100, unique_seed)
  ) |> 
  ungroup() |> 
  select(-combo_id, -unique_seed)  # Eliminar columnas temporales

sim_conditions <- sim_conditions |> 
  rowwise() |> 
  mutate(
    pre_loadings = list(loading_sim(n_items, type_loadings, seed_loadings)),
    thresholds = list(threshold_sim(
      n_items = n_items, 
      type_threshold = type_threshold, 
      n_cat = n_cat
    ))
  ) |> 
  select(-seed_loadings) |> 
  ungroup()

sim_conditions <- expand_grid(sim_conditions, n_replic = 1:500) |> 
  relocate(n_replic, .after = type_threshold) 
```


# Simulación

```{r}
generate_items <- function(n_size, distribution, n_items, pre_loadings, thresholds, n_cat) {
  # Generar el factor latente (F1)
  if (distribution == "MVN") {
    F1 <- rnorm(n_size)
  } else if (distribution == "Non-MVN") {
    F1 <- detectnorm::rnonnorm(n_size, skew = 7, kurt = 20)
    F1 <- F1$dat
  } else {
    stop("Invalid distribution type")
  }
  
  # Inicializar listas para almacenar los ítems generados
  items <- vector("list", n_items)
  items_cat <- vector("list", n_items)
  
  # Generar cada ítem
  for (i in seq_len(n_items)) {
    # Generar ítem continuo
    loading <- pre_loadings[i]
    error_sd <- sqrt(1 - loading^2)
    items[[i]] <- loading * F1 + error_sd * rnorm(n_size)
    
    # Categorizar el ítem usando los umbrales
    thresholds_i <- thresholds[[i]]
    
    # Validar que los thresholds coincidan con n_cat
    if (length(thresholds_i) + 1 != n_cat) {
      stop(glue::glue("Mismatch: n_cat = {n_cat}, thresholds = {length(thresholds_i)}"))
    }
    
    items_cat[[i]] <- cut(
      items[[i]], 
      breaks = c(-Inf, thresholds_i, Inf), 
      labels = seq_len(n_cat),
      right = FALSE
    )
  }
  
  # Combinar en un data frame
  items_df <- as.data.frame(do.call(cbind, items))
  colnames(items_df) <- paste0("x", seq_len(n_items))
  
  items_cat_df <- as.data.frame(do.call(cbind, items_cat))
  colnames(items_cat_df) <- paste0("x", seq_len(n_items), "_cat")
  
  # Retornar ambos conjuntos de datos
  return(cbind(items_df, items_cat_df))
}
```

```{r}
library(multidplyr)
cluster <- new_cluster(parallel::detectCores())
cluster_copy(cluster, "generate_items")
```

```{r}
sim_items_data <-  sim_conditions |> 
  rowwise() |> 
  partition(cluster) %>%
  mutate(
    sim_data = list(
      generate_items(n_size, distribution, n_items, pre_loadings, thresholds, n_cat)
    )
  )

sim_items_data <- sim_items_data |> 
  collect()
```

```{r}
saveRDS(sim_items_data, file = "sim_items_data.RData")
```


# Análisis

```{r}
sim_items_data <- readRDS("sim_items_data.RData")
```


## EFA

```{r}
get_correlation_matrix <- function(data, n_cat) {
  # Seleccionar columnas categorizadas
  cat_columns <- data[, grepl("_cat$", names(data))]
  
  if (n_cat == 5) {
    # Correlaciones policóricas
    result <- psych::polychoric(cat_columns)
  } else if (n_cat == 2) {
    # Correlaciones tetracóricas
    result <- psych::tetrachoric(cat_columns)
  } else {
    stop("Invalid number of categories. Must be 2 or 5.")
  }
  
  return(result)  # Devolver solo la matriz de correlaciones
}


run_efa <- function(data, corr_matrix, n_cat, n_size) {
  # Seleccionar columnas categorizadas
  cat_columns <- data[, grepl("_cat$", names(data))]
  
  # EFA 1: Con correlaciones Pearson (por defecto en psych::fa)
  efa_pearson <- psych::fa(cat_columns, nfactors = 1, n.obs = n_size, fm = "minres")
  
  # EFA 2: Con matriz de correlaciones policóricas o tetracóricas
  efa_categorical <- psych::fa(corr_matrix$rho, nfactors = 1, n.obs = n_size, fm = "minres")
  
  # Devolver resultados como una lista
  return(list(efa_pearson = efa_pearson, efa_categorical = efa_categorical))
}

```

```{r}
cluster_copy(cluster, "get_correlation_matrix")
cluster_copy(cluster, "run_efa")
```


```{r}
sim_items_analysis <- sim_items_data |> 
  rowwise() |> 
  partition(cluster) |> 
  mutate(
    corr_matrix = list(get_correlation_matrix(sim_data, n_cat)),
    efa_result = list(
      run_efa(sim_data, corr_matrix, n_cat, n_size)
    )
  ) |> 
  collect()
```


```{r}
saveRDS(sim_items_analysis, file = "sim_items_analysis.RData")
```

# Métricas

```{r}
sim_items_analysis <- readRDS("sim_items_analysis.RData")
```

## Agregar informacion de la correlacion teórica:

```{r}
adjust_theoretical_corr <- function(pre_loadings) {
  # Calcular la matriz de correlaciones como el producto externo de las cargas factoriales
  theoretical_matrix <- outer(pre_loadings, pre_loadings)
  
  # Asegurar que la diagonal sea exactamente 1
  diag(theoretical_matrix) <- 1
  
  return(theoretical_matrix)
}

process_estimated_thresholds <- function(estimated_tau, n_cat) {
  if (n_cat == 2) {
    # estimated_tau es un vector nombrado
    # Cada valor corresponde a un ítem
    estimated_thresholds_list <- lapply(names(estimated_tau), function(name) {
      estimated_tau[[name]]
    })
  } else if (n_cat == 5) {
    # estimated_tau es una matriz o data.frame
    # Cada fila corresponde a un ítem, y las columnas a los umbrales
    estimated_thresholds_list <- split(estimated_tau, rownames(estimated_tau))
  } else {
    stop("Número de categorías no soportado.")
  }
  return(estimated_thresholds_list)
}

# Alinear signos
align_sign <- function(estimated, theoretical) {
  cor_coef <- cor(estimated, theoretical)
  if (cor_coef < 0) {
    estimated <- -estimated
  }
  return(estimated)
}
```

```{r}
sim_items_analysis <- sim_items_analysis |> 
  rowwise() |> 
  mutate(
    corr_matrix_theo = list(adjust_theoretical_corr(pre_loadings)),
    thresholds_estimated = list(
        process_estimated_thresholds(
          corr_matrix$tau,
          n_cat
      )
    ),
    estimated_loadings_pearson = list(
      align_sign(
        efa_result$efa_pearson$loadings[,1],
        pre_loadings
      )
    ),
    estimated_loadings_cat = list(
      align_sign(
        efa_result$efa_categorical$loadings[,1],
        pre_loadings
      )
    )
  ) 
```

## Bias y MSE

```{r}
calculate_bias <- function(estimated, theoretical) {
  (estimated - theoretical) / theoretical
}

calculate_mse <- function(estimated, theoretical) {
  ((estimated - theoretical) / theoretical)^2
}


calculate_bias_thresholds <- function(estimated_thresholds, theoretical_thresholds) {
  # Inicializar una lista para almacenar los sesgos por réplica
  bias_list <- list()
  
  for (j in seq_along(theoretical_thresholds)) {  # Por cada ítem
    est_thresh <- estimated_thresholds[[j]]
    theo_thresh <- theoretical_thresholds[[j]]
    
    # Asegurarse de que los umbrales están alineados
    if (length(est_thresh) != length(theo_thresh)) {
      stop(paste("Desajuste en la cantidad de umbrales en el ítem", j))
    }
    
    # Calcular el sesgo relativo para cada umbral del ítem j
    bias_item <- (est_thresh - theo_thresh) / theo_thresh
    bias_list[[j]] <- bias_item
  }
  
  # Convertir la lista en un vector
  bias_vector <- mean(unlist(bias_list))
  return(bias_vector)
}

calculate_mse_thresholds <- function(estimated_thresholds, theoretical_thresholds) {
  # Similar a la función anterior, pero calcula el MSE
  mse_list <- list()
  
  for (j in seq_along(theoretical_thresholds)) {
    est_thresh <- estimated_thresholds[[j]]
    theo_thresh <- theoretical_thresholds[[j]]
    
    if (length(est_thresh) != length(theo_thresh)) {
      stop(paste("Desajuste en la cantidad de umbrales en el ítem", j))
    }
    
    mse_item <- ((est_thresh - theo_thresh) / theo_thresh)^2
    mse_list[[j]] <- mse_item
  }
  
  mse_vector <- mean(unlist(mse_list))
  return(mse_vector)
}

tucker_congruence <- function(estimated, theoretical) {
  sum(estimated * theoretical) / sqrt(sum(estimated^2) * sum(theoretical^2))
}
```

```{r}
cluster_copy(cluster, "calculate_bias")
cluster_copy(cluster, "calculate_mse")
cluster_copy(cluster, "calculate_bias_thresholds")
cluster_copy(cluster, "calculate_mse_thresholds")
cluster_copy(cluster, "tucker_congruence")

cluster_copy(cluster, "n")
```


```{r}
sim_items_analysis2 <- sim_items_analysis |> 
  rowwise() |> 
  partition(cluster) |> 
  # filter(
  #   n_size == 200,
  #   distribution == "MVN",
  #   n_items == 5,
  #   type_loadings == "high",
  #   n_cat %in% c(2, 5),
  #   type_threshold == "asym"
  # ) |>
  mutate(
    bias_loadings_pearson = mean(calculate_bias(estimated_loadings_pearson,
                                                pre_loadings )),
    bias_loadings_cat = mean(calculate_bias(estimated_loadings_cat,
                                            pre_loadings )),
    mse_loadings_pearson = mean(calculate_mse(estimated_loadings_pearson,
                                                pre_loadings )),
    mse_loadings_cat = mean(calculate_mse(estimated_loadings_cat,
                                            pre_loadings )),
    bias_threshold = calculate_bias_thresholds(thresholds_estimated,
                                               thresholds),
    mse_threshold = calculate_mse_thresholds(thresholds_estimated,
                                             thresholds),
    tucker_cong_pearson = tucker_congruence(estimated_loadings_pearson,
                                            pre_loadings),
    tucker_cong_cat = tucker_congruence(estimated_loadings_cat,
                                        pre_loadings)
  ) |> 
  collect()

sim_items_analysis3 <- sim_items_analysis2 |> 
  group_by(
    across(c(n_size:type_threshold, pre_loadings:thresholds))
  ) |> 
  summarise(
    # Sesgo de las cargas factoriales estimadas con Pearson
    bias_loadings_pearson = mean(bias_loadings_pearson, na.rm = TRUE),
    bias_loadings_pearson_lowconf = quantile(bias_loadings_pearson, 0.025, na.rm = TRUE),
    bias_loadings_pearson_highconf = quantile(bias_loadings_pearson, 0.975, na.rm = TRUE),
    
    # Sesgo de las cargas factoriales estimadas con métodos categóricos
    bias_loadings_cat = mean(bias_loadings_cat, na.rm = TRUE),
    bias_loadings_cat_lowconf = quantile(bias_loadings_cat, 0.025, na.rm = TRUE),
    bias_loadings_cat_highconf = quantile(bias_loadings_cat, 0.975, na.rm = TRUE),
    
    # MSE de las cargas factoriales estimadas con Pearson
    mse_loadings_pearson = mean(mse_loadings_pearson, na.rm = TRUE),
    mse_loadings_pearson_lowconf = quantile(mse_loadings_pearson, 0.025, na.rm = TRUE),
    mse_loadings_pearson_highconf = quantile(mse_loadings_pearson, 0.975, na.rm = TRUE),
    
    # MSE de las cargas factoriales estimadas con métodos categóricos
    mse_loadings_cat = mean(mse_loadings_cat, na.rm = TRUE),
    mse_loadings_cat_lowconf = quantile(mse_loadings_cat, 0.025, na.rm = TRUE),
    mse_loadings_cat_highconf = quantile(mse_loadings_cat, 0.975, na.rm = TRUE),
    
    # Sesgo de los umbrales
    bias_threshold = mean(bias_threshold, na.rm = TRUE),
    bias_threshold_lowconf = quantile(bias_threshold, 0.025, na.rm = TRUE),
    bias_threshold_highconf = quantile(bias_threshold, 0.975, na.rm = TRUE),
    
    # MSE de los umbrales
    mse_threshold = mean(mse_threshold, na.rm = TRUE),
    mse_threshold_lowconf = quantile(mse_threshold, 0.025, na.rm = TRUE),
    mse_threshold_highconf = quantile(mse_threshold, 0.975, na.rm = TRUE),
    
    # Coeficiente de congruencia de Tucker con Pearson
    tucker_cong_pearson = mean(tucker_cong_pearson, na.rm = TRUE),
    tucker_cong_pearson_lowconf = quantile(tucker_cong_pearson, 0.025, na.rm = TRUE),
    tucker_cong_pearson_highconf = quantile(tucker_cong_pearson, 0.975, na.rm = TRUE),
    
    # Coeficiente de congruencia de Tucker con métodos categóricos
    tucker_cong_cat = mean(tucker_cong_cat, na.rm = TRUE),
    tucker_cong_cat_lowconf = quantile(tucker_cong_cat, 0.025, na.rm = TRUE),
    tucker_cong_cat_highconf = quantile(tucker_cong_cat, 0.975, na.rm = TRUE),
    
    # Cantidad de réplicas en cada condición
    n_replicas = n()
  ) |> 
  ungroup()
```


```{r}
saveRDS(sim_items_analysis2, "sim_items_analysis2.RData")
```

```{r}
saveRDS(sim_items_analysis3, "sim_items_analysis3.RData")
```
